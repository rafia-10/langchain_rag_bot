ğŸ“š LangChain RAG Bot

A lightweight Retrieval-Augmented Generation (RAG) bot built on LangChain docs.
This assistant can answer technical questions such as:

â€œHow do I define a custom agent in LangChain?â€

â€œWhatâ€™s the role of memory in LangGraph?â€

It uses:

HuggingFace embeddings for semantic understanding

ChromaDB as a vector store with cosine similarity

Groq LLM (LLaMA-3) for fast, accurate responses

âœ¨ Purpose

The project demonstrates how to build a domain-specific knowledge assistant using only LangChainâ€™s official documentation (20â€“50 curated pages).
This keeps the knowledge base focused, efficient, and evaluator-friendly.

âš™ï¸ Methodology

Load Docs â†’ Fetch LangChain documentation pages.

Clean & Chunk â†’ Split docs into overlapping chunks for context.

Embed Text â†’ Use HuggingFace all-MiniLM-L6-v2 for embeddings.

Store in Vector DB â†’ Save embeddings in ChromaDB (cosine similarity).

Retrieve Relevant Chunks â†’ Query vector DB for relevant sections.

Generate Answer â†’ Use Groqâ€™s LLaMA-3 model with RAG context.

ğŸ“‹ Prerequisites

Python 3.9+

Virtual environment (venv) recommended

Groq API key (free)

ğŸ›  Installation
